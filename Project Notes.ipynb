{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathan.zhou/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/jonathan.zhou/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/jonathan.zhou/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import sys\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import itertools\n",
    "import sqlite3\n",
    "\n",
    "''' dependencies: sklearn, numpy, statsmodels '''\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n"
     ]
    }
   ],
   "source": [
    "db_path = \"ironate.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "comment_sep_str = \"\\n\\n\"+\"-\"*50+\"\\n\"\n",
    "\n",
    "def _make_sql_list_str(ls):\n",
    "    return \"(\" + \",\".join([str(x_i) for x_i in ls]) + \")\"\n",
    "\n",
    "labelers_of_interest = [2,4,5,6]\n",
    "labeler_id_str = _make_sql_list_str(labelers_of_interest)\n",
    "\n",
    "def _grab_single_element(result_set, COL=0):\n",
    "    return [x[COL] for x in result_set]\n",
    "\n",
    "def get_all_comment_ids():\n",
    "    return _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where labeler_id in %s;''' % \n",
    "                    labeler_id_str)) \n",
    "\n",
    "def get_ironic_comment_ids():\n",
    "    cursor.execute(\n",
    "        '''select distinct comment_id from irony_label \n",
    "            where forced_decision=0 and label=1 and labeler_id in %s;''' % \n",
    "            labeler_id_str)\n",
    "\n",
    "    ironic_comments = _grab_single_element(cursor.fetchall())\n",
    "    return ironic_comments\n",
    "\n",
    "def context_stats():\n",
    "    '''\n",
    "    Section 4, Eq (1) in the paper.\n",
    "\n",
    "    > irony_stats.context_stats()\n",
    "    ==============================================================================\n",
    "    Dep. Variable:                      y   No. Observations:                 3550\n",
    "    Model:                          Logit   Df Residuals:                     3548\n",
    "    Method:                           MLE   Df Model:                            1\n",
    "    Date:                Sat, 26 Apr 2014   Pseudo R-squ.:                 0.06012\n",
    "    Time:                        05:39:34   Log-Likelihood:                -2240.5\n",
    "    converged:                       True   LL-Null:                       -2383.8\n",
    "                                            LLR p-value:                 2.670e-64\n",
    "    ==============================================================================\n",
    "                     coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
    "    ------------------------------------------------------------------------------\n",
    "    const         -0.7108      0.040    -17.961      0.000        -0.788    -0.633\n",
    "    x1             1.5081      0.093     16.223      0.000         1.326     1.690\n",
    "    ==============================================================================\n",
    "    '''\n",
    "    all_comment_ids = get_all_comment_ids()\n",
    "\n",
    "    # pre-context / forced decisions\n",
    "    forced_decisions = _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where forced_decision=1 and labeler_id in %s;''' % \n",
    "                    labeler_id_str)) \n",
    "\n",
    "    for labeler in labelers_of_interest:\n",
    "        labeler_forced_decisions = _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where forced_decision=1 and labeler_id = %s;''' % \n",
    "                    labeler))\n",
    "\n",
    "        all_labeler_decisions = _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where forced_decision=0 and labeler_id = %s;''' % \n",
    "                    labeler))\n",
    "\n",
    "        p_labeler_forced = float(len(labeler_forced_decisions))/float(len(all_labeler_decisions))\n",
    "        print (\"labeler %s: %s\", labeler, p_labeler_forced)\n",
    "\n",
    "    p_forced = float(len(forced_decisions)) / float(len(all_comment_ids))\n",
    "\n",
    "    # now look at the proportion forced for the ironic comments\n",
    "    ironic_comments = get_ironic_comment_ids()\n",
    "    ironic_ids_str = _make_sql_list_str(ironic_comments)\n",
    "    forced_ironic_ids =  _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where \n",
    "                        forced_decision=1 and comment_id in %s and labeler_id in %s;''' % \n",
    "                                (ironic_ids_str, labeler_id_str))) \n",
    "\n",
    "    ''' regression bit: construct target vector + design matrix  '''\n",
    "    X,y = [],[]\n",
    "\n",
    "    for c_id in all_comment_ids:\n",
    "        if c_id in forced_decisions:\n",
    "            y.append(1.0)\n",
    "        else:\n",
    "            y.append(0.0)\n",
    "\n",
    "        if c_id in ironic_comments:\n",
    "            X.append([1.0])\n",
    "        else:\n",
    "            X.append([0.0])\n",
    "\n",
    "    X = sm.add_constant(X, prepend=True)\n",
    "    logit_mod = sm.Logit(y, X)\n",
    "    logit_res = logit_mod.fit()\n",
    "    \n",
    "    print (logit_res.summary())\n",
    "    return logit_res\n",
    "\n",
    "def ml_bow(show_features=False):\n",
    "    '''\n",
    "    Section 5, Eq (2) in the paper. \n",
    "\n",
    "    > irony_stats.ml_bow()\n",
    "    Optimization terminated successfully.\n",
    "             Current function value: 0.611578\n",
    "             Iterations 5\n",
    "                               Logit Regression Results                           \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                      y   No. Observations:                 1949\n",
    "    Model:                          Logit   Df Residuals:                     1946\n",
    "    Method:                           MLE   Df Model:                            2\n",
    "    Date:                Sun, 04 May 2014   Pseudo R-squ.:                 0.06502\n",
    "    Time:                        08:24:43   Log-Likelihood:                -1192.0\n",
    "    converged:                       True   LL-Null:                       -1274.9\n",
    "                                            LLR p-value:                 9.956e-37\n",
    "    ==============================================================================\n",
    "                     coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
    "    ------------------------------------------------------------------------------\n",
    "    const         -1.3284      0.088    -15.170      0.000        -1.500    -1.157\n",
    "    x1             0.9404      0.108      8.723      0.000         0.729     1.152\n",
    "    x2             0.7573      0.106      7.149      0.000         0.550     0.965\n",
    "    ==============================================================================\n",
    "\n",
    "    TWO NOTES:\n",
    "    1 A small bug in the original SQL code here resulted in a slightly different value for \n",
    "    x2; however the resutls are qualitatively the same as in the paper.\n",
    "    2 In any case, this result will vary slightly because we are using stochastic gradient \n",
    "    descent! Still, the x2 estimate and CI (which is of interest) should be quite close.\n",
    "    '''\n",
    "    all_comment_ids = get_labeled_thrice_comments()\n",
    "\n",
    "    ironic_comment_ids = get_ironic_comment_ids()\n",
    "    #ironic_ids_str = _make_sql_list_str(ironic_comments)\n",
    "\n",
    "    forced_decision_ids = _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where forced_decision=1 and labeler_id in %s;''' % \n",
    "                    labeler_id_str)) \n",
    "\n",
    "    comment_texts, y = [], []\n",
    "    for id_ in all_comment_ids:\n",
    "        comment_texts.append(grab_comments([id_])[0])\n",
    "        if id_ in ironic_comment_ids:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(-1)\n",
    "\n",
    "    # adding some features here; just adding them as tokens,\n",
    "    # which is admittedly kind of hacky.\n",
    "    emoticon_RE_str = '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    question_mark_RE_str = '\\?'\n",
    "    exclamation_point_RE_str = '\\!'\n",
    "    # any combination of multiple exclamation points and question marks\n",
    "    interrobang_RE_str = '[\\?\\!]{2,}'\n",
    "\n",
    "    for i, comment in enumerate(comment_texts):\n",
    "        #pdb.set_trace()\n",
    "        if len(re.findall(r'%s' % emoticon_RE_str, comment)) > 0:\n",
    "            comment = comment + \" PUNCxEMOTICON\"\n",
    "        if len(re.findall(r'%s' % exclamation_point_RE_str, comment)) > 0:\n",
    "            comment = comment + \" PUNCxEXCLAMATION_POINT\"\n",
    "        if len(re.findall(r'%s' % question_mark_RE_str, comment)) > 0:\n",
    "            comment = comment + \" PUNCxQUESTION_MARK\"\n",
    "        if len(re.findall(r'%s' % interrobang_RE_str, comment)) > 0:\n",
    "            comment = comment + \" PUNCxINTERROBANG\"\n",
    "        \n",
    "        if any([len(s) > 2 and str.isupper(s) for s in comment.split(\" \")]):\n",
    "            comment = comment + \" PUNCxUPPERCASE\" \n",
    "        \n",
    "        comment_texts[i] = comment\n",
    "    # vectorize\n",
    "    vectorizer = CountVectorizer(max_features=50000, ngram_range=(1,2), binary=True, stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(comment_texts)\n",
    "    kf = KFold(len(y), n_folds=5, shuffle=True)\n",
    "    X_context, y_mistakes = [], []\n",
    "    recalls, precisions = [], []\n",
    "    Fs = []\n",
    "    top_features = []\n",
    "    for train, test in kf:\n",
    "        train_ids = _get_entries(all_comment_ids, train)\n",
    "        test_ids = _get_entries(all_comment_ids, test)\n",
    "        y_train = _get_entries(y, train)\n",
    "        y_test = _get_entries(y, test)\n",
    "\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        svm = SGDClassifier(loss=\"hinge\", penalty=\"l2\", class_weight=\"auto\", alpha=.01)\n",
    "        #pdb.set_trace()\n",
    "        parameters = {'alpha':[.001, .01,  .1]}\n",
    "        clf = GridSearchCV(svm, parameters, scoring='f1')\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "        \n",
    "        #precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(y_test, preds)\n",
    "        tp, fp, tn, fn = 0,0,0,0\n",
    "        N = len(preds)\n",
    "\n",
    "        for i in xrange(N):\n",
    "            cur_id = test_ids[i]\n",
    "            irony_indicator = 1 if cur_id in ironic_comment_ids else 0\n",
    "            forced_decision_indicator = 1 if cur_id in forced_decision_ids else 0\n",
    "            # so x1 is the coefficient for forced decisions (i.e., context); \n",
    "            # x2 is the coeffecient for irony (overall)\n",
    "            X_context.append([irony_indicator, forced_decision_indicator])\n",
    "\n",
    "            y_i = y_test[i]\n",
    "            pred_y_i = preds[i]\n",
    "\n",
    "            if y_i == 1:\n",
    "                # ironic\n",
    "                if pred_y_i == 1:\n",
    "                    # true positive\n",
    "                    tp += 1 \n",
    "                    y_mistakes.append(0)\n",
    "                else:\n",
    "                    # false negative\n",
    "                    fn += 1\n",
    "                    y_mistakes.append(1)\n",
    "            else:\n",
    "                # unironic\n",
    "                if pred_y_i == -1:\n",
    "                    # true negative\n",
    "                    tn += 1\n",
    "                    y_mistakes.append(0)\n",
    "                else:\n",
    "                    # false positive\n",
    "                    fp += 1\n",
    "                    y_mistakes.append(1)\n",
    "\n",
    "        recall = tp/float(tp + fn)\n",
    "        precision = tp/float(tp + fp)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1 = 2* (precision * recall) / (precision + recall)\n",
    "        Fs.append(f1)\n",
    "\n",
    "    X_context = sm.add_constant(X_context, prepend=True)\n",
    "    logit_mod = sm.Logit(y_mistakes, X_context)\n",
    "    logit_res = logit_mod.fit()\n",
    "\n",
    "    print (logit_res.summary())\n",
    "\n",
    "def grab_comments(comment_id_list, verbose=False):\n",
    "    comments_list = []\n",
    "    for comment_id in comment_id_list:\n",
    "        cursor.execute(\"select text from irony_commentsegment where comment_id='%s' order by segment_index\" % comment_id)\n",
    "        segments = _grab_single_element(cursor.fetchall())\n",
    "        comment = \" \".join(segments)\n",
    "        if verbose:\n",
    "            print (comment)\n",
    "        comments_list.append(comment.encode('utf-8').strip())\n",
    "    return comments_list\n",
    "\n",
    "def _get_entries(a_list, indices):\n",
    "    return [a_list[i] for i in indices]\n",
    "\n",
    "def get_labeled_thrice_comments():\n",
    "    ''' get all ids for comments labeled >= 3 times '''\n",
    "    cursor.execute(\n",
    "        '''select comment_id from irony_label group by comment_id having count(distinct labeler_id) >= 3;'''\n",
    "    )\n",
    "    thricely_labeled_comment_ids = _grab_single_element(cursor.fetchall())\n",
    "    return thricely_labeled_comment_ids\n",
    "\n",
    "print (\"HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project References\n",
    "\n",
    "https://einstein.ai/static/images/pages/research/decaNLP/decaNLP.pdf - decaNLP\n",
    "https://arxiv.org/pdf/1611.01576.pdf - QRNN\n",
    "https://arxiv.org/abs/1902.02783 - \n",
    "\n",
    "Basic Sentiment analysis:\n",
    "http://www.aclweb.org/anthology/P02-1053.pdf\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.147.1344&rep=rep1&type=pdf\n",
    "https://www.ijert.org/research/a-survey-on-various-approaches-for-sentiment-analysis-and-performance-optimization-IJERTV6IS040525.pdf\n",
    "https://pdfs.semanticscholar.org/a65e/07deebf8a4e733ae5d9970d53e8555e948f0.pdf\n",
    "http://people.cs.pitt.edu/~wiebe/pubs/papers/somasundaranThesis.pdf\n",
    "http://www.coep.org.in/page_assets/341/121222013-DoddiKiran.pdf\n",
    "\n",
    "Applications/examples:\n",
    "Algorithmic trading: https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704\n",
    "ascribe.com products: https://goascribe.com/products-services/ascribe-cx-snapshot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
