{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project w266 Spring '19\n",
    "\n",
    "By Jonathan Zhou\n",
    "\n",
    "Dataset used can be found at https://github.com/bwallace/ACL-2014-irony\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathan.zhou/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/jonathan.zhou/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/jonathan.zhou/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import sys\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import itertools\n",
    "import sqlite3\n",
    "import string\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import LSTM, Embedding, Dropout, Input, TimeDistributed, Bidirectional, concatenate\n",
    "from keras.layers import Dense, Activation, Flatten, Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"ironate.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "comment_sep_str = \"\\n\\n\"+\"-\"*50+\"\\n\"\n",
    "\n",
    "def _make_sql_list_str(ls):\n",
    "    return \"(\" + \",\".join([str(x_i) for x_i in ls]) + \")\"\n",
    "\n",
    "labelers_of_interest = [2,4,5,6]\n",
    "labeler_id_str = _make_sql_list_str(labelers_of_interest)\n",
    "\n",
    "def _grab_single_element(result_set, COL=0):\n",
    "    return [x[COL] for x in result_set]\n",
    "\n",
    "def get_all_comment_ids():\n",
    "    return _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where labeler_id in %s;''' % \n",
    "                    labeler_id_str)) \n",
    "\n",
    "def get_ironic_comment_ids():\n",
    "    cursor.execute(\n",
    "        '''select distinct comment_id from irony_label \n",
    "            where label=1 and labeler_id in %s;''' % \n",
    "            labeler_id_str) \n",
    "\n",
    "    ironic_comments = _grab_single_element(cursor.fetchall())\n",
    "    return ironic_comments\n",
    "\n",
    "def context_stats():\n",
    "    all_comment_ids = get_all_comment_ids()\n",
    "\n",
    "    forced_decisions = _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where forced_decision=1 and labeler_id in %s;''' % \n",
    "                    labeler_id_str)) \n",
    "\n",
    "    for labeler in labelers_of_interest:\n",
    "        labeler_forced_decisions = _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where forced_decision=1 and labeler_id = %s;''' % \n",
    "                    labeler))\n",
    "\n",
    "        all_labeler_decisions = _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where forced_decision=0 and labeler_id = %s;''' % \n",
    "                    labeler))\n",
    "\n",
    "        p_labeler_forced = float(len(labeler_forced_decisions))/float(len(all_labeler_decisions))\n",
    "        print (\"labeler %s: %s\", labeler, p_labeler_forced)\n",
    "\n",
    "    p_forced = float(len(forced_decisions)) / float(len(all_comment_ids))\n",
    "\n",
    "    ironic_comments = get_ironic_comment_ids()\n",
    "    ironic_ids_str = _make_sql_list_str(ironic_comments)\n",
    "    forced_ironic_ids =  _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where \n",
    "                        forced_decision=1 and comment_id in %s and labeler_id in %s;''' % \n",
    "                                (ironic_ids_str, labeler_id_str))) \n",
    "\n",
    "    X,y = [],[]\n",
    "\n",
    "    for c_id in all_comment_ids:\n",
    "        if c_id in forced_decisions:\n",
    "            y.append(1.0)\n",
    "        else:\n",
    "            y.append(0.0)\n",
    "\n",
    "        if c_id in ironic_comments:\n",
    "            X.append([1.0])\n",
    "        else:\n",
    "            X.append([0.0])\n",
    "\n",
    "    X = sm.add_constant(X, prepend=True)\n",
    "    logit_mod = sm.Logit(y, X)\n",
    "    logit_res = logit_mod.fit()\n",
    "    \n",
    "    print (logit_res.summary())\n",
    "    return logit_res\n",
    "\n",
    "def grab_comments(comment_id_list, verbose=False):\n",
    "    comments_list = []\n",
    "    for comment_id in comment_id_list:\n",
    "        cursor.execute(\"select text from irony_commentsegment where comment_id='%s' order by segment_index\" % comment_id)\n",
    "        segments = _grab_single_element(cursor.fetchall())\n",
    "        comment = \" \".join(segments)\n",
    "        if verbose:\n",
    "            print (comment)\n",
    "        comments_list.append(comment.encode('utf-8').strip())\n",
    "    return comments_list\n",
    "\n",
    "def _get_entries(a_list, indices):\n",
    "    return [a_list[i] for i in indices]\n",
    "\n",
    "def get_labeled_thrice_comments():\n",
    "    ''' get all ids for comments labeled >= 3 times '''\n",
    "    cursor.execute(\n",
    "        '''select comment_id from irony_label group by comment_id having count(distinct labeler_id) >= 3;'''\n",
    "    )\n",
    "    thricely_labeled_comment_ids = _grab_single_element(cursor.fetchall())\n",
    "    return thricely_labeled_comment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformation (): \n",
    "    all_comment_ids = _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where labeler_id in %s;''' %labeler_id_str))\n",
    "\n",
    "    ironic_comment_ids = get_ironic_comment_ids()\n",
    "\n",
    "    forced_decision_ids = _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where forced_decision=1 and labeler_id in %s;''' % \n",
    "                    labeler_id_str))\n",
    "    \n",
    "    metadata = cursor.execute(\"select id, subreddit, redditor, upvotes, downvotes from irony_comment where id in (select distinct comment_id from irony_label);\")\n",
    "    metadata = [[x[0], x[1].encode('utf-8').strip(), x[2].encode('utf-8').strip(), x[3], 0, 0, 0, 0, 0] for x in metadata]\n",
    "\n",
    "    unique_users = [x[0].encode('utf-8').strip() for x in cursor.execute(\"select distinct redditor from irony_comment\")]\n",
    "    unique_subreddit = [x[0].encode('utf-8').strip() for x in cursor.execute(\"select distinct subreddit from irony_comment\")]\n",
    "    \n",
    "    for info in metadata: \n",
    "        info[2] = unique_users.index(info[2])\n",
    "        info[1] = unique_subreddit.index(info[1])\n",
    "    \n",
    "    metadata = np.array(metadata)\n",
    "        \n",
    "    comment_texts, y = [], []\n",
    "    for id_ in all_comment_ids:\n",
    "        comment_texts.append(grab_comments([id_])[0])\n",
    "        if id_ in ironic_comment_ids:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(-1)\n",
    "            \n",
    "    # adding some features here; just adding them as tokens,\n",
    "    # which is admittedly kind of hacky.\n",
    "    emoticon_RE_str = '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    question_mark_RE_str = '\\?'\n",
    "    exclamation_point_RE_str = '\\!'\n",
    "    # any combination of multiple exclamation points and question marks\n",
    "    interrobang_RE_str = '[\\?\\!]{2,}'\n",
    "\n",
    "    for i, comment in enumerate(comment_texts):\n",
    "        #pdb.set_trace()\n",
    "        if len(re.findall(r'%s' % emoticon_RE_str, comment)) > 0:\n",
    "            metadata[i][4] = 1\n",
    "        if len(re.findall(r'%s' % exclamation_point_RE_str, comment)) > 0:\n",
    "            metadata[i][5] = 1\n",
    "        if len(re.findall(r'%s' % question_mark_RE_str, comment)) > 0:\n",
    "            metadata[i][6] = 1\n",
    "        if len(re.findall(r'%s' % interrobang_RE_str, comment)) > 0:\n",
    "            metadata[i][7] = 1\n",
    "        if any([len(s) > 2 and str.isupper(s) for s in comment.split(\" \")]):\n",
    "            metadata[i][8] = 1\n",
    "        \n",
    "        comment = re.sub(r'\\d+', '', comment.lower())\n",
    "        comment_texts[i] = comment.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "            \n",
    "    return metadata, comment_texts, y\n",
    "\n",
    "def tokenizer(input_comments):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(input_comments)\n",
    "    sequences = tokenizer.texts_to_sequences(input_comments)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    return word_index, sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,4,5,6)\n",
      "Number of comments:  3550\n",
      "Number of ironic comments:  996\n",
      "[(11072,)]\n",
      "3550\n",
      "[(10039,)]\n",
      "6319\n"
     ]
    }
   ],
   "source": [
    "## Data Profiling\n",
    "print(labeler_id_str) # Unsure what this is for\n",
    "print('Number of comments: ', len(get_all_comment_ids())) # List of all comment IDs returned, 3550\n",
    "print('Number of ironic comments: ', len(get_ironic_comment_ids())) # List of all ironic comment ids, 723\n",
    "\n",
    "cursor.execute('''select count(*) from (select distinct segment_id from irony_label);''')\n",
    "print (cursor.fetchall())\n",
    "\n",
    "all_comment_ids = _grab_single_element(cursor.execute('''select distinct comment_id from irony_label'''))\n",
    "print (len(all_comment_ids))\n",
    "\n",
    "cursor.execute('''select count(*) from (select distinct id from irony_comment);''')\n",
    "print (cursor.fetchall())\n",
    "\n",
    "cursor.execute('''select redditor, count(*) from (select distinct id, redditor from irony_comment) group by redditor;''')\n",
    "print (len(cursor.fetchall()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline BOW Implementation from 2014 paper by Byron C. Wallace et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_bow(show_features=False):    \n",
    "    all_comment_ids = _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where labeler_id in %s;''' %labeler_id_str))\n",
    "\n",
    "    ironic_comment_ids = get_ironic_comment_ids()\n",
    "    #ironic_ids_str = _make_sql_list_str(ironic_comments)\n",
    "\n",
    "    forced_decision_ids = _grab_single_element(cursor.execute(\n",
    "                '''select distinct comment_id from irony_label where forced_decision=1 and labeler_id in %s;''' % \n",
    "                    labeler_id_str))\n",
    "\n",
    "    kf = KFold(len(y), n_folds=5, shuffle=True)\n",
    "    X_context, y_mistakes = [], []\n",
    "    recalls, precisions = [], []\n",
    "    Fs = []\n",
    "    top_features = []\n",
    "    for train, test in kf:\n",
    "        train_ids = _get_entries(all_comment_ids, train)\n",
    "        test_ids = _get_entries(all_comment_ids, test)\n",
    "        y_train = _get_entries(y, train)\n",
    "        y_test = _get_entries(y, test)\n",
    "\n",
    "        X_train, X_test = x[train], x[test]\n",
    "        svm = SGDClassifier(loss=\"hinge\", penalty=\"l2\", class_weight=\"balanced\", alpha=.01)\n",
    "        #pdb.set_trace()\n",
    "        parameters = {'alpha':[.001, .01,  .1]}\n",
    "        clf = GridSearchCV(svm, parameters, scoring='f1')\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "        \n",
    "        #precision, recall, f1, support = sklearn.metrics.precision_recall_fscore_support(y_test, preds)\n",
    "        tp, fp, tn, fn = 0,0,0,0\n",
    "        N = len(preds)\n",
    "\n",
    "        for i in xrange(N):\n",
    "            cur_id = test_ids[i]\n",
    "            irony_indicator = 1 if cur_id in ironic_comment_ids else 0\n",
    "            forced_decision_indicator = 1 if cur_id in forced_decision_ids else 0\n",
    "            # so x1 is the coefficient for forced decisions (i.e., context); \n",
    "            # x2 is the coeffecient for irony (overall)\n",
    "            X_context.append([irony_indicator, forced_decision_indicator])\n",
    "\n",
    "            y_i = y_test[i]\n",
    "            pred_y_i = preds[i]\n",
    "\n",
    "            if y_i == 1:\n",
    "                # ironic\n",
    "                if pred_y_i == 1:\n",
    "                    # true positive\n",
    "                    tp += 1 \n",
    "                    y_mistakes.append(0)\n",
    "                else:\n",
    "                    # false negative\n",
    "                    fn += 1\n",
    "                    y_mistakes.append(1)\n",
    "            else:\n",
    "                # unironic\n",
    "                if pred_y_i == -1:\n",
    "                    # true negative\n",
    "                    tn += 1\n",
    "                    y_mistakes.append(0)\n",
    "                else:\n",
    "                    # false positive\n",
    "                    fp += 1\n",
    "                    y_mistakes.append(1)\n",
    "\n",
    "        recall = tp/float(tp + fn)\n",
    "        precision = tp/float(tp + fp)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1 = 2* (precision * recall) / (precision + recall)\n",
    "        Fs.append(f1)\n",
    "\n",
    "    X_context = sm.add_constant(X_context, prepend=True)\n",
    "    logit_mod = sm.Logit(y_mistakes, X_context)\n",
    "    logit_res = logit_mod.fit()\n",
    "\n",
    "    print (logit_res.summary())\n",
    "    \n",
    "    print (Fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved BOW Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_bow(x, metadata, y, verbose=False):\n",
    "    x = np.concatenate((x, metadata), 1)\n",
    "    \n",
    "    indices = np.arange(x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * x.shape[0])\n",
    "\n",
    "    x_train = x[:-nb_validation_samples]\n",
    "    y_train = y[:-nb_validation_samples]\n",
    "    x_val = x[-nb_validation_samples:]\n",
    "    y_val = y[-nb_validation_samples:]\n",
    "\n",
    "    svm = SGDClassifier(loss=\"hinge\", penalty=\"l2\", class_weight=\"balanced\", alpha=.01)\n",
    "    parameters = {'alpha':[.00001, .0001, .001, .01,  .1]}\n",
    "    clf = GridSearchCV(svm, parameters, scoring='accuracy')\n",
    "    clf.fit(x_train, y_train)\n",
    "    \n",
    "    if verbose: \n",
    "        fn = 0\n",
    "        fp = 0\n",
    "        y_preds = clf.predict(x_val)\n",
    "\n",
    "        for i, comment in enumerate(x_val):\n",
    "            if y_preds[i] != y_val[i]: \n",
    "                print ('-----PREDICTED =', y_preds[i], '------ACTUAL =', y_val[i])\n",
    "                print (comment)\n",
    "            if y_preds[i] == 1 and y_val[i] == -1: \n",
    "                fp += 1\n",
    "            if y_preds[i] == -1 and y_val[i] == 1: \n",
    "                fn += 1\n",
    "              \n",
    "        print ('false positives =', fp)\n",
    "        print ('false negatives =', fn)\n",
    "    \n",
    "    return clf.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    embedding_matrix = np.random.random((len(V) + 1, EMBEDDING_DIM))\n",
    "\n",
    "    inputs = Input(name='inputs',shape=[MAX_SEQUENCE_LENGTH])\n",
    "    meta_inputs = Input(name='meta_inputs',shape=[9])\n",
    "    embed = Embedding(len(V) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                      input_length=MAX_SEQUENCE_LENGTH, trainable=True)(inputs)\n",
    "    layer = Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.1, kernel_regularizer=l2(0.01)))(embed)\n",
    "    layer = Dense(60,name='FC1')(layer)\n",
    "    layer = Activation('tanh')(layer)\n",
    "    layer = concatenate([layer, meta_inputs])\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('softmax')(layer)\n",
    "    \n",
    "    return Model(inputs=[inputs, meta_inputs], outputs=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(): \n",
    "    embedding_matrix = np.random.random((len(V) + 1, EMBEDDING_DIM))\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    meta_inputs = Input(name='meta_inputs',shape=[9])\n",
    "    embedded_sequences = Embedding(len(V) + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                                   input_length=MAX_SEQUENCE_LENGTH, trainable=True)(sequence_input)\n",
    "    l_cov1= Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "    l_cov2 = Conv1D(64, 5, activation='relu')(l_pool1)\n",
    "    l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "    l_cov3 = Conv1D(64, 5, activation='relu')(l_pool2)\n",
    "    l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "    l_flat = Flatten()(l_pool3)\n",
    "    layer = concatenate([l_flat, meta_inputs])\n",
    "    layer = Dense(1,activation='softmax')(layer)\n",
    "    return Model(inputs=[sequence_input, meta_inputs], outputs=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.666518\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                 3550\n",
      "Model:                          Logit   Df Residuals:                     3547\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Fri, 12 Apr 2019   Pseudo R-squ.:                 0.03598\n",
      "Time:                        23:27:34   Log-Likelihood:                -2366.1\n",
      "converged:                       True   LL-Null:                       -2454.5\n",
      "                                        LLR p-value:                 4.401e-39\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.3590      0.046      7.725      0.000       0.268       0.450\n",
      "x1            -1.0706      0.088    -12.206      0.000      -1.242      -0.899\n",
      "x2             0.1330      0.079      1.691      0.091      -0.021       0.287\n",
      "==============================================================================\n",
      "[0.3185840707964602, 0.4585492227979275, 0.3877221324717286, 0.43360433604336046, 0.39182282793867124]\n"
     ]
    }
   ],
   "source": [
    "# Baseline BOW Training\n",
    "ml_bow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 250, 30)      401310      inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128)          48640       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "FC1 (Dense)                     (None, 60)           7740        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 60)           0           FC1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "meta_inputs (InputLayer)        (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 69)           0           activation_3[0][0]               \n",
      "                                                                 meta_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "out_layer (Dense)               (None, 1)            70          concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1)            0           out_layer[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 457,760\n",
      "Trainable params: 457,760\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 2840 samples, validate on 710 samples\n",
      "Epoch 1/10\n",
      "2840/2840 [==============================] - 23s 8ms/step - loss: 22.2536 - acc: 0.3204 - val_loss: 28.3218 - val_acc: 0.1211\n",
      "Epoch 2/10\n",
      "2840/2840 [==============================] - 19s 7ms/step - loss: 21.8307 - acc: 0.3204 - val_loss: 28.0830 - val_acc: 0.1211\n",
      "Epoch 3/10\n",
      "2840/2840 [==============================] - 19s 7ms/step - loss: 21.6914 - acc: 0.3204 - val_loss: 28.0254 - val_acc: 0.1211\n",
      "Epoch 4/10\n",
      "2840/2840 [==============================] - 15s 5ms/step - loss: 21.6687 - acc: 0.3204 - val_loss: 28.0227 - val_acc: 0.1211\n",
      "Epoch 5/10\n",
      "2840/2840 [==============================] - 15s 5ms/step - loss: 21.6682 - acc: 0.3204 - val_loss: 28.0227 - val_acc: 0.1211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3a53f190>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LSTM Training\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "MAX_NB_WORDS = 2000\n",
    "EMBEDDING_DIM = 30\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "metadata, comments, y = data_transformation()\n",
    "V, x = tokenizer(comments)\n",
    "x = pad_sequences(x, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y = np.asarray(y)\n",
    "\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "model.fit([x, metadata], y, batch_size=BATCH_SIZE, epochs=10,\n",
    "          validation_split=VALIDATION_SPLIT, callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "\n",
    "# .2866 / 22.746 = 250/2000/20\n",
    "# .3204 / 21.6682 = 250/2000/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1000, 10)     133770      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 996, 64)      3264        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 199, 64)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 195, 64)      20544       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 39, 64)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 35, 64)       20544       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 64)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "meta_inputs (InputLayer)        (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 73)           0           flatten_1[0][0]                  \n",
      "                                                                 meta_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            74          concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 178,196\n",
      "Trainable params: 178,196\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 2840 samples, validate on 710 samples\n",
      "Epoch 1/10\n",
      "2840/2840 [==============================] - 6s 2ms/step - loss: 21.6682 - acc: 0.3204 - val_loss: 28.0227 - val_acc: 0.1211\n",
      "Epoch 2/10\n",
      "2840/2840 [==============================] - 5s 2ms/step - loss: 21.6682 - acc: 0.3204 - val_loss: 28.0227 - val_acc: 0.1211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c301b5b10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN Execution\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 500\n",
    "EMBEDDING_DIM = 10\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "metadata, comments, y = data_transformation()\n",
    "V, x = tokenizer(comments)\n",
    "x = pad_sequences(x, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y = np.asarray(y)\n",
    "\n",
    "model = CNN()\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "model.fit([x, metadata], y, batch_size=BATCH_SIZE, epochs=10,\n",
    "          validation_split=VALIDATION_SPLIT, callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "\n",
    "# .3204 / 21.6682 = 1000/500/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy for updated BOW = 0.6867605633802818\n",
      "Accuracy scores = [0.6239436619718309, 0.7225352112676057, 0.6352112676056338, 0.7380281690140845, 0.7140845070422536]\n"
     ]
    }
   ],
   "source": [
    "# Improved BOW Training\n",
    "score_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    score_list.append(new_bow(x, metadata, y))\n",
    "    \n",
    "print ('Average accuracy for updated BOW =', sum(score_list) / len(score_list))\n",
    "print ('Accuracy scores =', score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
